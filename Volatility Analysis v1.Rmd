

Import the libraries

```{r}
library("quantmod")
library("purrr")
library("rugarch")
library("rmgarch")
library("igraph")
library("reshape2")
library("ggplot2")
library(sna)
library(tsna)
library(ndtv)
```




Import the symbols and returns data. As the dataset contains NA values corresponding to the non-trading holidays, we use na.omit to remove those rows from the dataset. 

```{r}
nifty50 <- read.csv(url("https://www1.nseindia.com/content/indices/ind_nifty50list.csv"))

symbols <- as.vector(paste0(nifty50$Symbol, ".NS"))

# Step 1: Download historical prices dataset from Yahoo Finance

getSymbols(symbols,
           from = "2013-01-01",
           to = "2020-04-30", src="yahoo")

prices <- data.frame(map(symbols,function(x) Ad(get(x))))

prices <- na.omit(prices)

# prices <- reduce(prices,merge)

logPrices <- log(prices)


# taking difference of logs of prices for calculating returns
ret <- na.omit(diff(as.matrix(logPrices)))

# Drop all the dataframes of individual stock information
rm(list = ls(pattern = ".NS"))

## Step 2: Convert daily historical data to weekly

wkPrices <- apply.weekly(prices, colMeans)

logWkPrices <- log(wkPrices)

wkRet <- diff(as.matrix(logWkPrices))


```
To keep the datasize small, we use weekly returns. We also assume the same univariate model applies to all the constituent stocks for simplicity.


```{r}
#same univariate model for all 50 stocks
returns = wkRet

uspec.n = multispec(replicate(50,ugarchspec(mean.model = list(armaOrder = c(1,0)))))

multf = multifit(uspec.n, returns)

spec1 = dccspec(uspec = uspec.n, dccOrder = c(1,1), distribution = "mvnorm")


fit1 = dccfit(spec1, data=returns, fit.control = list(eval.se = TRUE), fit=multf)

cor1 = rcor(fit1)
```

We melt the correlation matrix into its long form so as to plot the histogram of correlations. This encompassess all pairwise correlations for all trading days in the period 01JAN2013 - 30APR2020.  

```{r}
long <- melt(cor1)

ggplot(data=long, aes(value)) + 
  geom_histogram(col="red", 
                 fill="green", 
                 alpha = .2) + 
  labs(title="Histogram for Correlation") +
  labs(x="Correlation", y="Freq")
```


save.image(file="volatility_analysis.RData")

```{r}
load("C:/Users/bagre/Documents/Volatility NIFTY/volatility_analysis.RData")
```
```{r}
med <- print(median(long$value))
```

Constructing networks for each day


Constructing the network for the first day
```{r}
clust.avg <- c()
clust.global <- c()
edge.ratio <- c()
new.links.formed <- c()
prev.edges <- c()
#for ( i in dim(cor1)[3]){
# Correlation matrix for day 1:
links <- as.matrix(cor1[, , 1])
links[links < med] <- 0

# Maximum eigen value of cor matrix
#max(eigen(links)$values)

net <- graph_from_adjacency_matrix(links, mode = "undirected", weighted = T, diag = F, add.colnames = T, add.rownames = F)

#Calculate graph metrics
clust.avg <- append(clust.avg,transitivity(net, type = "average"))
clust.global <- append(clust.global,transitivity(net))
edge.ratio <- append(edge.ratio, length(E(net))/1225)
new.links.formed <- append(new.links.formed, length(setdiff(E(net),prev.edges)) + length(setdiff(prev.edges,E(net))))
prev.edges <- E(net)
#}
```


```{r}
for ( i in 2:dim(cor1)[3]){
# Correlation matrix for day i:
links <- as.matrix(cor1[, , i])
links[links < med] <- 0

# Maximum eigen value of cor matrix
#max(eigen(links)$values)

net <- graph_from_adjacency_matrix(links, mode = "undirected", weighted = T, diag = F, add.colnames = T, add.rownames = F)
#Calculate graph metrics
clust.avg <- append(clust.avg,transitivity(net, type = "average"))
clust.global <- append(clust.global,transitivity(net))
edge.ratio <- append(edge.ratio, length(E(net))/1225)
new.links.formed <- append(new.links.formed, length(setdiff(E(net),prev.edges)) + length(setdiff(prev.edges,E(net))))
prev.edges <- E(net)
}
```

```{r}
start.date <- as.Date("2013-01-01")
end.date <- as.Date("2020-04-30")
dates <- seq(start.date,end.date, by = "weeks")
dates <- dates[2:length(dates)]
```

```{r}
graph.metrics <- data.frame(date = dates,
                            average.clustering = clust.avg,
                            global.clustering = clust.global,
                            median.edge.ratio = edge.ratio,
                            new.links.formed = new.links.formed)
```

```{r}
ggplot() + geom_line(data = graph.metrics[2:nrow(graph.metrics),], aes(x = date, y = new.links.formed)) + scale_x_date(date_labels = "%Y-%b", breaks = "year")
```

```{r}
ggplot() + geom_line(data = graph.metrics, aes(x = date, y = median.edge.ratio)) + scale_x_date(date_labels = "%Y-%b", breaks = "year")
```

```{r}
ggplot() + geom_line(data = graph.metrics, aes(x = date, y = average.clustering)) + scale_x_date(date_labels = "%Y-%b", breaks = "year")
```

```{r}
ggplot() + geom_line(data = graph.metrics, aes(x = date, y = global.clustering)) + scale_x_date(date_labels = "%Y-%b", breaks = "year")
```

Comparing the metrics with NIFTYVIX
```{r}
vix <- read.csv("NIFTYVIX.csv")
vix <- xts(vix, as.Date(vix$Date, "%d-%m-%Y"))
VIX.weekly <- data.frame(apply.weekly(vix$NIFTYVIX, mean))
VIX.weekly$Date <- as.Date(row.names(VIX.weekly))
VIX.weekly <- VIX.weekly[VIX.weekly$Date %in% start.date:end.date,]
VIX.weekly <- VIX.weekly[2:nrow(VIX.weekly),]
```

```{r}
reg <- lm(VIX.weekly$NIFTYVIX~graph.metrics$average.clustering + graph.metrics$global.clustering)
summary(reg)
```

```{r}
reg <- lm(VIX.weekly$NIFTYVIX~graph.metrics$median.edge.ratio)
summary(reg)
```

```{r}
reg <- lm(VIX.weekly$NIFTYVIX~graph.metrics$new.links.formed)
summary(reg)
```

```{r}
reg <- lm(VIX.weekly$NIFTYVIX~graph.metrics$median.edge.ratio + graph.metrics$average.clustering)
summary(reg)
```